{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXM5evY13+K1GIo0AQU9nN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "daa7bcdcfea54f169be69950a9464417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dbc6415d836046678300c4ce0f54f336",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_952a8af3aca4427a999773b2e5281ca1",
              "IPY_MODEL_68dc4170759e411c900f67be1f4a6970"
            ]
          }
        },
        "dbc6415d836046678300c4ce0f54f336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "952a8af3aca4427a999773b2e5281ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3acc73d10a154c7bb98f90157f9c095e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a043dd42a9e24d1ab3a7a6d6fe9a6c40"
          }
        },
        "68dc4170759e411c900f67be1f4a6970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36ac1518b0664aaa9d12dc316bfec0ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [04:24&lt;00:00,  3.78s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4cd767464bb4b8e9b6581eba8660c46"
          }
        },
        "3acc73d10a154c7bb98f90157f9c095e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a043dd42a9e24d1ab3a7a6d6fe9a6c40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36ac1518b0664aaa9d12dc316bfec0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4cd767464bb4b8e9b6581eba8660c46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edolardizzone/Homework2-Caltech101/blob/master/per_ale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-N3w6ewsef3",
        "outputId": "9b5af571-fc6a-4669-d64b-801d40b3ad3e"
      },
      "source": [
        "!pip3 install 'import_ipynb'\r\n",
        "!pip3 install 'tqdm'\r\n",
        " \r\n",
        "!rm -r IncrementalLearning\r\n",
        "# upload work files from your git hub repository\r\n",
        "import sys\r\n",
        " \r\n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\r\n",
        "!rm -rf IncrementalLearning/README.md \r\n",
        "!rm -rf IncrementalLearning/baselines.ipynb\r\n",
        " \r\n",
        "path = 'IncrementalLearning/'\r\n",
        "if path not in sys.path:\r\n",
        "    sys.path.append('IncrementalLearning/')\r\n",
        " \r\n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=ecb44a5f53e7289b76b76170a7a2176211fa8cb47a2829a04bfa303a504d4f08\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "rm: cannot remove 'IncrementalLearning': No such file or directory\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (162/162), done.\u001b[K\n",
            "remote: Total 637 (delta 100), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (637/637), 831.40 KiB | 8.15 MiB/s, done.\n",
            "Resolving deltas: 100% (375/375), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "z5qpHXZ0sVmG",
        "outputId": "2408b77d-7990-475d-f042-c87b391cfcee"
      },
      "source": [
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "from __future__ import unicode_literals\r\n",
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import math\r\n",
        "from sklearn.preprocessing import normalize\r\n",
        "import copy\r\n",
        "import torchvision.datasets as dsets\r\n",
        "import torchvision.models as models\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.gridspec as gridspec\r\n",
        "from torch.utils.data import Subset, DataLoader\r\n",
        "import random\r\n",
        "from sklearn.metrics import confusion_matrix as s_cm\r\n",
        "import seaborn as sn\r\n",
        "import pandas as pd\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "# import torch.nn.parallel\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.optim as optim\r\n",
        "# import torch.utils.data\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.utils as vutils\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import collections\r\n",
        "\r\n",
        "import import_ipynb\r\n",
        "from IncrementalLearning.cifar100 import ilCIFAR100\r\n",
        "\r\n",
        "from IncrementalLearning.resnet_cifar import resnet32\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n",
        "try:\r\n",
        "    from apex.parallel import DistributedDataParallel as DDP\r\n",
        "    from apex import amp, optimizers\r\n",
        "    USE_APEX = True\r\n",
        "except ImportError:\r\n",
        "    print(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\r\n",
        "    print(\"will attempt to run without it\")\r\n",
        "    USE_APEX = False\r\n",
        "\r\n",
        "#provide intermeiate information\r\n",
        "debug_output = False\r\n",
        "debug_output = True\r\n",
        "\r\n",
        "\r\n",
        "class DeepInversionFeatureHook():\r\n",
        "    '''\r\n",
        "    Implementation of the forward hook to track feature statistics and compute a loss on them.\r\n",
        "    Will compute mean and variance, and will use l2 as a loss\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, module):\r\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\r\n",
        "\r\n",
        "    def hook_fn(self, module, input, output):\r\n",
        "        # hook co compute deepinversion's feature distribution regularization\r\n",
        "        nch = input[0].shape[1]\r\n",
        "\r\n",
        "        mean = input[0].mean([0, 2, 3])\r\n",
        "        var = input[0].permute(1, 0, 2, 3).contiguous().view([nch, -1]).var(1, unbiased=False)\r\n",
        "\r\n",
        "        # forcing mean and variance to match between two distributions\r\n",
        "        # other ways might work better, e.g. KL divergence\r\n",
        "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(\r\n",
        "            module.running_mean.data.type(var.type()) - mean, 2)\r\n",
        "\r\n",
        "        self.r_feature = r_feature\r\n",
        "        # must have no output\r\n",
        "\r\n",
        "    def close(self):\r\n",
        "        self.hook.remove()\r\n",
        "\r\n",
        "def get_images(net, bs=128, epochs=1000, idx=-1, var_scale=0.00005,\r\n",
        "               net_student=None, prefix=None, competitive_scale=0.01, train_writer = None, global_iteration=None,\r\n",
        "               use_amp=False,\r\n",
        "               optimizer = None, inputs = None, bn_reg_scale = 0.0, random_labels = False, l2_coeff=0.0):\r\n",
        "    '''\r\n",
        "    Function returns inverted images from the pretrained model, parameters are tight to CIFAR dataset\r\n",
        "    args in:\r\n",
        "        net: network to be inverted\r\n",
        "        bs: batch size\r\n",
        "        epochs: total number of iterations to generate inverted images, training longer helps a lot!\r\n",
        "        idx: an external flag for printing purposes: only print in the first round, set as -1 to disable\r\n",
        "        var_scale: the scaling factor for variance loss regularization. this may vary depending on bs\r\n",
        "            larger - more blurred but less noise\r\n",
        "        net_student: model to be used for Adaptive DeepInversion\r\n",
        "        prefix: defines the path to store images\r\n",
        "        competitive_scale: coefficient for Adaptive DeepInversion\r\n",
        "        train_writer: tensorboardX object to store intermediate losses\r\n",
        "        global_iteration: indexer to be used for tensorboard\r\n",
        "        use_amp: boolean to indicate usage of APEX AMP for FP16 calculations - twice faster and less memory on TensorCores\r\n",
        "        optimizer: potimizer to be used for model inversion\r\n",
        "        inputs: data place holder for optimization, will be reinitialized to noise\r\n",
        "        bn_reg_scale: weight for r_feature_regularization\r\n",
        "        random_labels: sample labels from random distribution or use columns of the same class\r\n",
        "        l2_coeff: coefficient for L2 loss on input\r\n",
        "    return:\r\n",
        "        A tensor on GPU with shape (bs, 3, 32, 32) for CIFAR\r\n",
        "    '''\r\n",
        "\r\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean').cuda()\r\n",
        "    \r\n",
        "    # preventing backpropagation through student for Adaptive DeepInversion\r\n",
        "    net_student.eval()\r\n",
        "\r\n",
        "    best_cost = 1e6\r\n",
        "\r\n",
        "    # initialize gaussian inputs\r\n",
        "    inputs.data = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda')\r\n",
        "    # if use_amp:\r\n",
        "    #     inputs.data = inputs.data.half()\r\n",
        "\r\n",
        "    # set up criteria for optimization\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    optimizer.state = collections.defaultdict(dict)  # Reset state of optimizer\r\n",
        "\r\n",
        "    # target outputs to generate\r\n",
        "    if random_labels:\r\n",
        "        targets = torch.LongTensor([random.randint(0,9) for _ in range(bs)]).to('cuda')\r\n",
        "    else:\r\n",
        "        targets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]).to('cuda')\r\n",
        "\r\n",
        "    ## Create hooks for feature statistics catching\r\n",
        "    loss_r_feature_layers = []\r\n",
        "    for module in net.modules():\r\n",
        "        if isinstance(module, nn.BatchNorm2d):\r\n",
        "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\r\n",
        "\r\n",
        "    # setting up the range for jitter\r\n",
        "    lim_0, lim_1 = 2, 2\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "        # apply random jitter offsets\r\n",
        "        off1 = random.randint(-lim_0, lim_0)\r\n",
        "        off2 = random.randint(-lim_1, lim_1)\r\n",
        "        inputs_jit = torch.roll(inputs, shifts=(off1,off2), dims=(2,3))\r\n",
        "\r\n",
        "        # foward with jit images\r\n",
        "        optimizer.zero_grad()\r\n",
        "        net.zero_grad()\r\n",
        "        outputs = net(inputs_jit)\r\n",
        "        loss = criterion(outputs, targets)\r\n",
        "        loss_target = loss.item()\r\n",
        "\r\n",
        "        # competition loss, Adaptive DeepInvesrion\r\n",
        "        if competitive_scale != 0.0:\r\n",
        "            net_student.zero_grad()\r\n",
        "            outputs_student = net_student(inputs_jit)\r\n",
        "            T = 3.0\r\n",
        "\r\n",
        "            if 1:\r\n",
        "                # jensen shanon divergence:\r\n",
        "                # another way to force KL between negative probabilities\r\n",
        "                P = F.softmax(outputs_student / T, dim=1)\r\n",
        "                Q = F.softmax(outputs / T, dim=1)\r\n",
        "                M = 0.5 * (P + Q)\r\n",
        "\r\n",
        "                P = torch.clamp(P, 0.01, 0.99)\r\n",
        "                Q = torch.clamp(Q, 0.01, 0.99)\r\n",
        "                M = torch.clamp(M, 0.01, 0.99)\r\n",
        "                eps = 0.0\r\n",
        "                # loss_verifier_cig = 0.5 * kl_loss(F.log_softmax(outputs_verifier / T, dim=1), M) +  0.5 * kl_loss(F.log_softmax(outputs/T, dim=1), M)\r\n",
        "                loss_verifier_cig = 0.5 * kl_loss(torch.log(P + eps), M) + 0.5 * kl_loss(torch.log(Q + eps), M)\r\n",
        "                # JS criteria - 0 means full correlation, 1 - means completely different\r\n",
        "                loss_verifier_cig = 1.0 - torch.clamp(loss_verifier_cig, 0.0, 1.0)\r\n",
        "\r\n",
        "                loss = loss + competitive_scale * loss_verifier_cig\r\n",
        "\r\n",
        "        # apply total variation regularization\r\n",
        "        diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\r\n",
        "        diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\r\n",
        "        diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\r\n",
        "        diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\r\n",
        "        loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\r\n",
        "        loss = loss + var_scale*loss_var\r\n",
        "\r\n",
        "        # R_feature loss\r\n",
        "        loss_distr = sum([mod.r_feature for mod in loss_r_feature_layers])\r\n",
        "        loss = loss + bn_reg_scale*loss_distr # best for noise before BN\r\n",
        "\r\n",
        "        # l2 loss\r\n",
        "        if 1:\r\n",
        "            loss = loss + l2_coeff * torch.norm(inputs_jit, 2)\r\n",
        "\r\n",
        "        if debug_output and epoch % 200==0:\r\n",
        "            print(f\"It {epoch}\\t Losses: total: {loss.item():3.3f},\\ttarget: {loss_target:3.3f} \\tR_feature_loss unscaled:\\t {loss_distr.item():3.3f}\")\r\n",
        "            vutils.save_image(inputs.data.clone(),\r\n",
        "                              './{}/output_{}.png'.format(prefix, epoch//200),\r\n",
        "                              normalize=True, scale_each=True, nrow=10)\r\n",
        "\r\n",
        "        if best_cost > loss.item():\r\n",
        "            best_cost = loss.item()\r\n",
        "            best_inputs = inputs.data\r\n",
        "\r\n",
        "        # backward pass\r\n",
        "        if use_amp:\r\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\r\n",
        "                scaled_loss.backward()\r\n",
        "        else:\r\n",
        "            loss.backward()\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "    outputs=net(best_inputs)\r\n",
        "    _, predicted_teach = outputs.max(1)\r\n",
        "\r\n",
        "    outputs_student=net_student(best_inputs)\r\n",
        "    _, predicted_std = outputs_student.max(1)\r\n",
        "\r\n",
        "    if idx == 0:\r\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\r\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\r\n",
        "\r\n",
        "    name_use = \"best_images\"\r\n",
        "    if prefix is not None:\r\n",
        "        name_use = prefix + name_use\r\n",
        "    next_batch = len(glob.glob(\"./%s/*.png\" % name_use)) // 1\r\n",
        "\r\n",
        "    vutils.save_image(best_inputs[:20].clone(),\r\n",
        "                      './{}/output_{}.png'.format(name_use, next_batch),\r\n",
        "                      normalize=True, scale_each = True, nrow=10)\r\n",
        "\r\n",
        "    if train_writer is not None:\r\n",
        "        train_writer.add_scalar('gener_teacher_criteria', criterion(outputs, targets), global_iteration)\r\n",
        "        train_writer.add_scalar('gener_student_criteria', criterion(outputs_student, targets), global_iteration)\r\n",
        "\r\n",
        "        train_writer.add_scalar('gener_teacher_acc', predicted_teach.eq(targets).sum().item() / bs, global_iteration)\r\n",
        "        train_writer.add_scalar('gener_student_acc', predicted_std.eq(targets).sum().item() / bs, global_iteration)\r\n",
        "\r\n",
        "        train_writer.add_scalar('gener_loss_total', loss.item(), global_iteration)\r\n",
        "        train_writer.add_scalar('gener_loss_var', (var_scale*loss_var).item(), global_iteration)\r\n",
        "\r\n",
        "    net_student.train()\r\n",
        "\r\n",
        "    return best_inputs\r\n",
        "\r\n",
        "\r\n",
        "def test():\r\n",
        "    print('==> Teacher validation')\r\n",
        "    net_teacher.eval()\r\n",
        "    test_loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\r\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\r\n",
        "            outputs = net_teacher(inputs)\r\n",
        "            loss = criterion(outputs, targets)\r\n",
        "\r\n",
        "            test_loss += loss.item()\r\n",
        "            _, predicted = outputs.max(1)\r\n",
        "            total += targets.size(0)\r\n",
        "            correct += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "    print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\r\n",
        "          % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\r\n",
        "\r\n",
        "'''\r\n",
        "if __name__ == \"__main__\":\r\n",
        "\r\n",
        "    parser = argparse.ArgumentParser(description='PyTorch CIFAR10 DeepInversion')\r\n",
        "    parser.add_argument('--bs', default=256, type=int, help='batch size')\r\n",
        "    parser.add_argument('--iters_mi', default=2000, type=int, help='number of iterations for model inversion')\r\n",
        "    parser.add_argument('--cig_scale', default=0.0, type=float, help='competition score')\r\n",
        "    parser.add_argument('--di_lr', default=0.1, type=float, help='lr for deep inversion')\r\n",
        "    parser.add_argument('--di_var_scale', default=2.5e-5, type=float, help='TV L2 regularization coefficient')\r\n",
        "    parser.add_argument('--di_l2_scale', default=0.0, type=float, help='L2 regularization coefficient')\r\n",
        "    parser.add_argument('--r_feature_weight', default=1e2, type=float, help='weight for BN regularization statistic')\r\n",
        "    parser.add_argument('--amp', action='store_true', help='use APEX AMP O1 acceleration')\r\n",
        "    parser.add_argument('--exp_descr', default=\"try1\", type=str, help='name to be added to experiment name')\r\n",
        "    parser.add_argument('--teacher_weights', default=\"'./checkpoint/teacher_resnet34_only.weights'\", type=str, help='path to load weights of the model')\r\n",
        "\r\n",
        "    args = parser.parse_args()\r\n",
        "\r\n",
        "    print(\"loading resnet32\")\r\n",
        "\r\n",
        "    net_teacher = ResNet()\r\n",
        "    net_student = ResNet()\r\n",
        "\r\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "    net_student = net_student.to(device)\r\n",
        "    net_teacher = net_teacher.to(device)\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    # place holder for inputs\r\n",
        "    data_type = torch.half if args.amp else torch.float\r\n",
        "    inputs = torch.randn((args.bs, 3, 32, 32), requires_grad=True, device='cuda', dtype=data_type)\r\n",
        "\r\n",
        "    optimizer_di = optim.Adam([inputs], lr=args.di_lr)\r\n",
        "\r\n",
        "    if args.amp:\r\n",
        "        opt_level = \"O1\"\r\n",
        "        loss_scale = 'dynamic'\r\n",
        "\r\n",
        "        [net_student, net_teacher], optimizer_di = amp.initialize(\r\n",
        "            [net_student, net_teacher], optimizer_di,\r\n",
        "            opt_level=opt_level,\r\n",
        "            loss_scale=loss_scale)\r\n",
        "\r\n",
        "    checkpoint = torch.load(args.teacher_weights)\r\n",
        "    net_teacher.load_state_dict(checkpoint)\r\n",
        "    net_teacher.eval() #important, otherwise generated images will be non natural\r\n",
        "    if args.amp:\r\n",
        "        # need to do this trick for FP16 support of batchnorms\r\n",
        "        net_teacher.train()\r\n",
        "        for module in net_teacher.modules():\r\n",
        "            if isinstance(module, nn.BatchNorm2d):\r\n",
        "                module.eval().half()\r\n",
        "\r\n",
        "    cudnn.benchmark = True\r\n",
        "\r\n",
        "\r\n",
        "    batch_idx = 0\r\n",
        "    prefix = \"runs/data_generation/\"+args.exp_descr+\"/\"\r\n",
        "\r\n",
        "    for create_folder in [prefix, prefix+\"/best_images/\"]:\r\n",
        "        if not os.path.exists(create_folder):\r\n",
        "            os.makedirs(create_folder)\r\n",
        "\r\n",
        "    if 0:\r\n",
        "        # loading\r\n",
        "        transform_test = transforms.Compose([\r\n",
        "            transforms.ToTensor(),\r\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n",
        "        ])\r\n",
        "\r\n",
        "        original_test_set = ilCIFAR100(10,203, train=False)\r\n",
        "        testloader = torch.utils.data.DataLoader(original_test_set, batch_size=args.bs, shuffle=True, num_workers=6,\r\n",
        "                                                 drop_last=True)\r\n",
        "        # Checking teacher accuracy\r\n",
        "        print(\"Checking teacher accuracy\")\r\n",
        "        test()\r\n",
        "\r\n",
        "\r\n",
        "    train_writer = None  # tensorboard writter\r\n",
        "    global_iteration = 0\r\n",
        "\r\n",
        "    print(\"Starting model inversion\")\r\n",
        "\r\n",
        "    inputs = get_images(net=net_teacher, bs=args.bs, epochs=args.iters_mi, idx=batch_idx,\r\n",
        "                        net_student=net_student, prefix=prefix, competitive_scale=args.cig_scale,\r\n",
        "                        train_writer=train_writer, global_iteration=global_iteration, use_amp=args.amp,\r\n",
        "                        optimizer=optimizer_di, inputs=inputs, bn_reg_scale=args.r_feature_weight,\r\n",
        "                        var_scale=args.di_var_scale, random_labels=False, l2_coeff=args.di_l2_scale)\r\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please install apex from https://www.github.com/nvidia/apex to run this example.\n",
            "will attempt to run without it\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n\\n    parser = argparse.ArgumentParser(description=\\'PyTorch CIFAR10 DeepInversion\\')\\n    parser.add_argument(\\'--bs\\', default=256, type=int, help=\\'batch size\\')\\n    parser.add_argument(\\'--iters_mi\\', default=2000, type=int, help=\\'number of iterations for model inversion\\')\\n    parser.add_argument(\\'--cig_scale\\', default=0.0, type=float, help=\\'competition score\\')\\n    parser.add_argument(\\'--di_lr\\', default=0.1, type=float, help=\\'lr for deep inversion\\')\\n    parser.add_argument(\\'--di_var_scale\\', default=2.5e-5, type=float, help=\\'TV L2 regularization coefficient\\')\\n    parser.add_argument(\\'--di_l2_scale\\', default=0.0, type=float, help=\\'L2 regularization coefficient\\')\\n    parser.add_argument(\\'--r_feature_weight\\', default=1e2, type=float, help=\\'weight for BN regularization statistic\\')\\n    parser.add_argument(\\'--amp\\', action=\\'store_true\\', help=\\'use APEX AMP O1 acceleration\\')\\n    parser.add_argument(\\'--exp_descr\\', default=\"try1\", type=str, help=\\'name to be added to experiment name\\')\\n    parser.add_argument(\\'--teacher_weights\\', default=\"\\'./checkpoint/teacher_resnet34_only.weights\\'\", type=str, help=\\'path to load weights of the model\\')\\n\\n    args = parser.parse_args()\\n\\n    print(\"loading resnet32\")\\n\\n    net_teacher = ResNet()\\n    net_student = ResNet()\\n\\n    device = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\n\\n    net_student = net_student.to(device)\\n    net_teacher = net_teacher.to(device)\\n\\n    criterion = nn.CrossEntropyLoss()\\n\\n    # place holder for inputs\\n    data_type = torch.half if args.amp else torch.float\\n    inputs = torch.randn((args.bs, 3, 32, 32), requires_grad=True, device=\\'cuda\\', dtype=data_type)\\n\\n    optimizer_di = optim.Adam([inputs], lr=args.di_lr)\\n\\n    if args.amp:\\n        opt_level = \"O1\"\\n        loss_scale = \\'dynamic\\'\\n\\n        [net_student, net_teacher], optimizer_di = amp.initialize(\\n            [net_student, net_teacher], optimizer_di,\\n            opt_level=opt_level,\\n            loss_scale=loss_scale)\\n\\n    checkpoint = torch.load(args.teacher_weights)\\n    net_teacher.load_state_dict(checkpoint)\\n    net_teacher.eval() #important, otherwise generated images will be non natural\\n    if args.amp:\\n        # need to do this trick for FP16 support of batchnorms\\n        net_teacher.train()\\n        for module in net_teacher.modules():\\n            if isinstance(module, nn.BatchNorm2d):\\n                module.eval().half()\\n\\n    cudnn.benchmark = True\\n\\n\\n    batch_idx = 0\\n    prefix = \"runs/data_generation/\"+args.exp_descr+\"/\"\\n\\n    for create_folder in [prefix, prefix+\"/best_images/\"]:\\n        if not os.path.exists(create_folder):\\n            os.makedirs(create_folder)\\n\\n    if 0:\\n        # loading\\n        transform_test = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\\n        ])\\n\\n        original_test_set = ilCIFAR100(10,203, train=False)\\n        testloader = torch.utils.data.DataLoader(original_test_set, batch_size=args.bs, shuffle=True, num_workers=6,\\n                                                 drop_last=True)\\n        # Checking teacher accuracy\\n        print(\"Checking teacher accuracy\")\\n        test()\\n\\n\\n    train_writer = None  # tensorboard writter\\n    global_iteration = 0\\n\\n    print(\"Starting model inversion\")\\n\\n    inputs = get_images(net=net_teacher, bs=args.bs, epochs=args.iters_mi, idx=batch_idx,\\n                        net_student=net_student, prefix=prefix, competitive_scale=args.cig_scale,\\n                        train_writer=train_writer, global_iteration=global_iteration, use_amp=args.amp,\\n                        optimizer=optimizer_di, inputs=inputs, bn_reg_scale=args.r_feature_weight,\\n                        var_scale=args.di_var_scale, random_labels=False, l2_coeff=args.di_l2_scale)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2hxrMtSUMJa"
      },
      "source": [
        "class icarl(nn.Module):\r\n",
        "  def __init__(self,n_classes=100):\r\n",
        "    super(icarl, self).__init__()\r\n",
        "    self.model = resnet32(num_classes=n_classes)\r\n",
        "    self.feature_extractor = self.model.features\r\n",
        "    self.lr = 2\r\n",
        "    self.gamma = 0.2\r\n",
        "    self.weight_decay =1e-5 \r\n",
        "    self.milestones = [49,63]\r\n",
        "    self.batch_size = 128\r\n",
        "    self.numepochs = 70\r\n",
        "    self.n_classes = 0\r\n",
        "    self.n_known = 0\r\n",
        "    self.feature_size=64\r\n",
        "    self.momentum=0.9\r\n",
        "    self.criterion=nn.BCEWithLogitsLoss()\r\n",
        "    self.compute_means = True\r\n",
        "    self.exemplar_means = []\r\n",
        "    self.exemplar_sets = []\r\n",
        "    self.NUM_BATCHES=10\r\n",
        "    self.randomseed=203\r\n",
        "    self.trainloader=None\r\n",
        "    self.testloader=None\r\n",
        "    self.CLASSES_PER_BATCH=10\r\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed)\r\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train=False)\r\n",
        "    self.device='cuda'\r\n",
        "    self.classes_seen=0\r\n",
        "    self.diz = self.original_training_set.get_dict()\r\n",
        "\r\n",
        "  def update_parameters(self):\r\n",
        "    old_model = copy.deepcopy(self.model)\r\n",
        "   \r\n",
        "    old_model.eval()\r\n",
        "    old_model.to(self.device)\r\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\r\n",
        "    print(n_classes)\r\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\r\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\r\n",
        "    for epoch in tqdm(range(self.numepochs)):\r\n",
        "        \r\n",
        "      for _, inputs, labels in self.trainloader:\r\n",
        "        inputs = inputs.float().to(self.device)\r\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\r\n",
        "\r\n",
        "        labels=labels.to(self.device)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        outputs=self.model(inputs)\r\n",
        "\r\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device) #CAMBIARE ONE_HOT\r\n",
        "        \r\n",
        "        if self.classes_seen:\r\n",
        "          old_target = old_model(inputs).to(self.device)\r\n",
        "          old_target = torch.sigmoid(old_target).to(self.device)\r\n",
        "          \r\n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\r\n",
        "          loss = self.criterion(outputs, target)\r\n",
        "        else:\r\n",
        "          loss = self.criterion(outputs, labels_encoded) \r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "      \r\n",
        "      scheduler.step()\r\n",
        "  def get_new_exemplars(self,batch,m):\r\n",
        "    '''\r\n",
        "    loader=torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\r\n",
        "    features = np.zeros((0,self.feature_size))\r\n",
        "    indices = np.zeros((0), dtype=int)\r\n",
        "    with torch.no_grad():\r\n",
        "      for indexes,images,labels in loader:\r\n",
        "        images = images.cuda()\r\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\r\n",
        "        feature = normalize(feature, axis=1, norm='l2')\r\n",
        "        features = np.concatenate((features,feature), axis=0)\r\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\r\n",
        "\r\n",
        "    class_mean = np.mean(features, axis=0)\r\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\r\n",
        "\r\n",
        "    exemplar_set = []\r\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\r\n",
        "\r\n",
        "    for k in range(1, int(m)+1):\r\n",
        "        S = np.sum(exemplar_features, axis=0)\r\n",
        "        phi = features\r\n",
        "        mu = class_mean\r\n",
        "        mu_p = 1.0 / k * (phi + S)\r\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\r\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\r\n",
        "        exemplar_set.append(indices[i])\r\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\r\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\r\n",
        "\r\n",
        "        #remove duplicates\r\n",
        "        features = np.delete(features, i, 0)\r\n",
        "        indices = np.delete(indices, i, 0)\r\n",
        "        \r\n",
        "    self.exemplar_sets.append(exemplar_set)\r\n",
        "\r\n",
        "    '''\r\n",
        "    \r\n",
        "    bs=256\r\n",
        "    iters_mi=1000\r\n",
        "    cig_scale=0.0\r\n",
        "    di_lr=0.1\r\n",
        "    di_var_scale=2.5e-5\r\n",
        "    di_l2_scale=0.0\r\n",
        "    r_feature_weight=1e2\r\n",
        "    amp=False\r\n",
        "    exp_descr='try1'\r\n",
        "    teacher_weights=\"'./checkpoint/teacher_resnetcifar_only.weights'\"\r\n",
        "    \r\n",
        "    icriterion = nn.CrossEntropyLoss()\r\n",
        "    net_teacher=self.model\r\n",
        "    net_student=self.model\r\n",
        "    # place holder for inputs\r\n",
        "    data_type = torch.half if amp else torch.float\r\n",
        "    inputs = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda', dtype=data_type)\r\n",
        "\r\n",
        "    optimizer_di = optim.Adam([inputs], lr=di_lr)\r\n",
        "\r\n",
        "    if amp:\r\n",
        "        opt_level = \"O1\"\r\n",
        "        loss_scale = 'dynamic'\r\n",
        "\r\n",
        "        [net_student, net_teacher], optimizer_di = amp.initialize(\r\n",
        "            [net_student, net_teacher], optimizer_di,\r\n",
        "            opt_level=opt_level,\r\n",
        "            loss_scale=loss_scale)\r\n",
        "\r\n",
        "    \r\n",
        "    net_teacher.eval() #important, otherwise generated images will be non natural\r\n",
        "    if amp:\r\n",
        "        # need to do this trick for FP16 support of batchnorms\r\n",
        "        net_teacher.train()\r\n",
        "        for module in net_teacher.modules():\r\n",
        "            if isinstance(module, nn.BatchNorm2d):\r\n",
        "                module.eval().half()\r\n",
        "\r\n",
        "    cudnn.benchmark = True\r\n",
        "\r\n",
        "\r\n",
        "    batch_idx = 0\r\n",
        "    prefix = \"runs/data_generation/\"+exp_descr+\"/\"\r\n",
        "\r\n",
        "    for create_folder in [prefix, prefix+\"/best_images/\"]:\r\n",
        "        if not os.path.exists(create_folder):\r\n",
        "            os.makedirs(create_folder)\r\n",
        "\r\n",
        "    if 0:\r\n",
        "        # loading\r\n",
        "        transform_test = transforms.Compose([\r\n",
        "            transforms.ToTensor(),\r\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\r\n",
        "        ])\r\n",
        "\r\n",
        "        original_test_set = ilCIFAR100(10,203, train=False)\r\n",
        "\r\n",
        "        testloader = torch.utils.data.DataLoader(original_test_set, batch_size=bs, shuffle=True, num_workers=6,\r\n",
        "                                                 drop_last=True)\r\n",
        "        # Checking teacher accuracy\r\n",
        "        print(\"Checking teacher accuracy\")\r\n",
        "        test()\r\n",
        "\r\n",
        "\r\n",
        "    train_writer = None  # tensorboard writter\r\n",
        "    global_iteration = 0\r\n",
        "\r\n",
        "    print(\"Starting model inversion\")\r\n",
        "\r\n",
        "    images_new = get_images(net=net_teacher, bs=bs, epochs=iters_mi, idx=batch_idx,\r\n",
        "                        net_student=net_student, prefix=prefix, competitive_scale=cig_scale,\r\n",
        "                        train_writer=train_writer, global_iteration=global_iteration, use_amp=amp,\r\n",
        "                        optimizer=optimizer_di, inputs=inputs, bn_reg_scale=r_feature_weight,\r\n",
        "                        var_scale=di_var_scale, random_labels=False, l2_coeff=di_l2_scale)\r\n",
        "    plt.imshow(inputs[1].permute(1,2,0).numpy())\r\n",
        "\r\n",
        "  def reduce_old_exemplars(self,m):\r\n",
        "    for y, P_y in enumerate(self.exemplar_sets):\r\n",
        "            self.exemplar_sets[y] = P_y[:int(m)]\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    self.model = self.model.cuda()\r\n",
        "    return self.model.forward(x)\r\n",
        "  def classify(self, image):\r\n",
        "    _, preds = torch.max(torch.softmax(self.forward(image), dim=1), dim=1, keepdim=False)\r\n",
        "    return preds\r\n",
        "  def __accuracy_on(self, dl, model, mapper):\r\n",
        "    total = 0.0\r\n",
        "    correct = 0.0\r\n",
        "    for  _,images, labels in dl:\r\n",
        "        labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\r\n",
        "        images = Variable(images).cuda()\r\n",
        "        preds = model.classify(images)\r\n",
        "        total = total + len(labels)\r\n",
        "        correct += (preds.data.cpu() == labels).sum()\r\n",
        "\r\n",
        "    acc = 100 * correct / total\r\n",
        "    return acc\r\n",
        "  def training_model(self):\r\n",
        "    \r\n",
        "    train_indices = self.original_training_set.get_batch_indexes()\r\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\r\n",
        "    batches=self.original_training_set.getbatches()\r\n",
        "    current_test_indexes=[]\r\n",
        "    acc=[]\r\n",
        "    accuracy=0\r\n",
        "    for i in range(self.NUM_BATCHES):\r\n",
        "      \r\n",
        "      for exemplars in self.exemplar_sets:\r\n",
        "        train_indices[i]=np.concatenate([train_indices[i], np.array(exemplars)])\r\n",
        "      train_dataset=Subset(self.original_training_set,train_indices[i])\r\n",
        "      current_test_indexes+=test_indices[i].tolist()\r\n",
        "      test_dataset=Subset(self.original_test_set,current_test_indexes)\r\n",
        "      self.trainloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\r\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)        \r\n",
        "      self.train()\r\n",
        "      self.update_parameters()\r\n",
        "      print('si dio icarl')      \r\n",
        "      self.classes_seen+=10\r\n",
        "      self.eval() # Set Network to evaluation mode\r\n",
        "      print('accuracy on training set:', self.__accuracy_on(self.trainloader,self,self.diz))\r\n",
        "      print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\r\n",
        "      print('-' * 80)\r\n",
        "      acc.append(accuracy)\r\n",
        "      m=int(2000/(int(i*10+10)))\r\n",
        "      self.reduce_old_exemplars(m)\r\n",
        "      torch.save(self.model,\"./checkpoint/teacher_resnetcifar_only.weights\")\r\n",
        "      for classlabel in batches[i]:\r\n",
        "        indexes_class=self.original_training_set.get_class_indexes(classlabel)\r\n",
        "        current_class=Subset(self.original_training_set,indexes_class)\r\n",
        "        self.get_new_exemplars(indexes_class,m)\r\n",
        "    return self"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627,
          "referenced_widgets": [
            "daa7bcdcfea54f169be69950a9464417",
            "dbc6415d836046678300c4ce0f54f336",
            "952a8af3aca4427a999773b2e5281ca1",
            "68dc4170759e411c900f67be1f4a6970",
            "3acc73d10a154c7bb98f90157f9c095e",
            "a043dd42a9e24d1ab3a7a6d6fe9a6c40",
            "36ac1518b0664aaa9d12dc316bfec0ff",
            "d4cd767464bb4b8e9b6581eba8660c46"
          ]
        },
        "id": "jrM4B4p7sW-R",
        "outputId": "667dd881-c376-40a8-c509-8046941e5759"
      },
      "source": [
        "model=icarl().cuda()\r\n",
        "model.training_model()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "daa7bcdcfea54f169be69950a9464417",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "si dio icarl\n",
            "accuracy on training set: tensor(98.3774)\n",
            "accuracy on test set: tensor(83.3705)\n",
            "--------------------------------------------------------------------------------\n",
            "Starting model inversion\n",
            "It 0\t Losses: total: 25217.352,\ttarget: 20.823 \tR_feature_loss unscaled:\t 251.964\n",
            "It 200\t Losses: total: 2693.817,\ttarget: 10.095 \tR_feature_loss unscaled:\t 26.836\n",
            "It 400\t Losses: total: 1944.368,\ttarget: 10.133 \tR_feature_loss unscaled:\t 19.341\n",
            "It 600\t Losses: total: 3033.233,\ttarget: 10.736 \tR_feature_loss unscaled:\t 30.224\n",
            "It 800\t Losses: total: 1812.990,\ttarget: 9.838 \tR_feature_loss unscaled:\t 18.031\n",
            "Teacher correct out of 256: 34, loss at 10.206274032592773\n",
            "Student correct out of 256: 34, loss at 10.206274032592773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-0cbac0e4a080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0micarl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-a3f4b053a1e2>\u001b[0m in \u001b[0;36mtraining_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mindexes_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_training_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_class_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasslabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mcurrent_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_training_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindexes_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_new_exemplars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-a3f4b053a1e2>\u001b[0m in \u001b[0;36mget_new_exemplars\u001b[0;34m(self, batch, m)\u001b[0m\n\u001b[1;32m    175\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_di\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_reg_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr_feature_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                         var_scale=di_var_scale, random_labels=False, l2_coeff=di_l2_scale)\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce_old_exemplars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "zO1SMEDdVqqe",
        "outputId": "bac9061c-aa9f-4271-9aab-c366334ba20d"
      },
      "source": [
        "%tb"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0cbac0e4a080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0micarl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-3992a57aab46>\u001b[0m in \u001b[0;36mtraining_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mindexes_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_training_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_class_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasslabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mcurrent_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_training_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindexes_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_new_exemplars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-3992a57aab46>\u001b[0m in \u001b[0;36mget_new_exemplars\u001b[0;34m(self, batch, m)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--teacher_weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"'./checkpoint/teacher_resnet34_only.weights'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'path to load weights of the model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0micriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mnet_teacher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1746\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1747\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2402\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpGwzWppaO3V",
        "outputId": "6342fd09-bb25-4c16-b467-ca29b99ded96"
      },
      "source": [
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GIUykrif-DR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}